{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gradientai\n",
      "  Downloading gradientai-1.4.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aenum>=3.1.11 (from gradientai)\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /home/sacha924/.local/lib/python3.8/site-packages (from gradientai) (1.10.7)\n",
      "Collecting python-dateutil>=2.8.2 (from gradientai)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25.3 in /home/sacha924/.local/lib/python3.8/site-packages (from gradientai) (2.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/sacha924/.local/lib/python3.8/site-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sacha924/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->gradientai) (1.12.0)\n",
      "Downloading gradientai-1.4.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.1/171.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aenum, python-dateutil, gradientai\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.0\n",
      "    Uninstalling python-dateutil-2.8.0:\n",
      "      Successfully uninstalled python-dateutil-2.8.0\n",
      "Successfully installed aenum-3.1.15 gradientai-1.4.0 python-dateutil-2.8.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradientai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GRADIENT_ACCESS_TOKEN'] = \"xxx\"\n",
    "os.environ['GRADIENT_WORKSPACE_ID'] = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model adapter with id 379a4323-b96f-46cf-bc3e-a3eb625b4c20_model_adapter\n",
      "Asking: ### Instruction: Write the Python Code for the following exercise : Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice.You can return the answer in any order. \n",
      "\n",
      "### Response:\n",
      "Generated (before fine-tune): \n",
      " Here's the Python code for the given exercise:\n",
      "```python\n",
      "def twoSum(nums, target):\n",
      "    numDict = {}\n",
      "    for i, num in enumerate(nums):\n",
      "        if target - num in numDict:\n",
      "            return [numDict[target-num], i]\n",
      "        numDict[num] = i\n",
      "    return []\n",
      "```\n",
      "This function takes in an array of integers `nums` and an integer `target`\n"
     ]
    }
   ],
   "source": [
    "from gradientai import Gradient\n",
    "\n",
    "def main():\n",
    "  with Gradient() as gradient:\n",
    "      base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\")\n",
    "\n",
    "      new_model_adapter = base_model.create_model_adapter(\n",
    "          name=\"test model 3\"\n",
    "      )\n",
    "      print(f\"Created model adapter with id {new_model_adapter.id}\")\n",
    "      sample_query = \"### Instruction: Write the Python Code for the following exercise : Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice.You can return the answer in any order. \\n\\n### Response:\"\n",
    "      print(f\"Asking: {sample_query}\")\n",
    "\n",
    "      # before fine-tuning\n",
    "      completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
    "      print(f\"Generated (before fine-tune): {completion}\")\n",
    "\n",
    "      # samples = [\n",
    "      #   { \"inputs\": \"### Instruction: Who is Matthew Berman? \\n\\n### Response: Matthew Berman is a popular video creator who talks about AI\" },\n",
    "      #   { \"inputs\": \"### Instruction: Who is the person named Matthew Berman ? \\n\\n### Response: Matthew Berman is a YouTuber who talks about AI\" },\n",
    "      #   { \"inputs\": \"### Instruction: Can you tell me about Matthew Berman? \\n\\n### Response: Matthew Berman is a popular creator who specializes in AI content\" },\n",
    "      # ]\n",
    "\n",
    "      # this is where fine-tuning happens\n",
    "      # num_epochs is the number of times you fine-tune the model\n",
    "      # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
    "      # play around with this number to find what works best for you\n",
    "      # num_epochs = 3\n",
    "      # count = 0\n",
    "      # while count < num_epochs:\n",
    "      #     print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
    "      #     new_model_adapter.fine_tune(samples=samples)\n",
    "      #     count = count + 1\n",
    "\n",
    "      # after fine-tuning\n",
    "      # completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
    "      # print(f\"Generated (after fine-tune): {completion}\")\n",
    "\n",
    "      # new_model_adapter.delete()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
